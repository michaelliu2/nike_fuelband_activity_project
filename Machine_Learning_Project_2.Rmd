#Machine Learning Project

First, we import the data. We also partition our data into a train and validation set and set our last set as our test set.  We set our seed for the whole analysis and open the necessary libraries.

```{r ,cache=TRUE}
setInternet2(TRUE)
data=read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
training=data;
test=read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
library(caret)
library(klaR)
library(rpart)

inTrain=createDataPartition(y=training$classe,p=.75,list=FALSE)
train=training[inTrain,]
validation=training[-inTrain,]
set.seed(25)

```

##Exploratory Analysis:

Here, we see that our different classes are mostly evenly distributed except with more in class A.

Also, we do a density plot and a plot of total_accel_dumbell versus roll_belt with classe indicated by color.  We see that it is hard to distinguish between classes based on total_accel_dumbell, but roll_belt variable seems to distinguish class E somewhat from the rest of the variables.

```{r, cache=TRUE}
table(train$classe)
qplot(total_accel_dumbbell,colour=classe,data=train,geom="density")
qplot(total_accel_dumbbell,roll_belt,colour=classe,data=train)
```

##Preprocess:

We preprocess a little by first deleting any variables from our data that probably won't be useful.  We start by deleting any columns with too many NA's.  Then we delete the names column because we shouldn't be predicting based on what person is doing the activity. Next, we look at our variables and notice that the variables that relate to the forearm, arm, dumbbell, or belt are probably the most relevant while other variables like time and date probably aren't.  We only take the variables with the aforementioned words. Last, we take out any variables that have too little variance to further trim down.

```{r, cache=TRUE}
sums=sapply(train,function(x){sum(is.na(x))})  ##delete variables with too many NAs
train=train[,sums<10]

train=train[,!colnames(train) %in% "user_name"] ##delete names column
index=grep("_forearm|_arm|_dumbbell|_belt|classe",names(train)) ## only take columns with following words
train=train[,index]

nsv=nearZeroVar(train,saveMetrics=TRUE) ##take out variables with too little variance
train=train[,!nsv$nzv]
```

###Select predictors for model

We do a simple binary tree against our outcome "classe", using only 1 predictor at at time to see if some predictors could be more useful than others.  We split our train data with a k-split.  Then we train the data on each train split and test on each test split.  We then average our accuracies to get a better cross-validation estimate of the accuracy. We notice that each variable has around the same accuracy, although _belt variables score the highest, around 40%.  

```{r, cache=TRUE}
set.seed(25)
variable_scores=vector("list",ncol(train)-1)
names(variable_scores)=names(train[,!names(train) %in% "classe"])
for (name in names(train[,!names(train) %in% "classe"])){
        folds=createFolds(y=train$classe,k=5,returnTrain=TRUE)
        sums=0
        for(i in 1:2){
        data=train[,c(name[[1]],"classe")]
        obj=train(data$classe~.,data=data,method="rpart")
        data2=train[-folds[[1]],]
        pred=predict(obj,data2)
        accuracy=sum(pred==train[-folds[[1]],"classe"])/nrow(train[-folds[[1]],])
        sums=sums+accuracy
        }
        variable_scores[name[[1]]]=sums/2
        
}
variable_scores=unlist(variable_scores)
variable_scores=sort(variable_scores,decreasing=TRUE) ##We note there are no variables that particularly stand out to eliminate, although _belt variables seem to be the most important


names=names(train) ##here are the names of columns that we keep; we delete them from our test and validation data too
test=test[,names(test) %in% names]
validation=validation[,names(validation) %in% c(names,"problem_id")]
```

Next, we run PCA variables (variable names with "arm", "forearm", "belt", or "dumbbell" in column names).  We also create PCA  sets for our test and validation.  With "PCA", we capture 95% variance from each group.  In total, we end with around 30 predictors from an initial 159.

```{r, cache=TRUE}
##Preprocessing: PCA to reduce number of variables further, dividing by each body part, because we assume each body part is correlated

##_arm variables PCA
preProc_arm=preProcess(train[,grep("_arm",colnames(train))],method="pca")
pred_arm=predict(preProc_arm,train[,grep("_arm",colnames(train))])
pred_arm_test=predict(preProc_arm,test[,grep("_arm",colnames(test))])
pred_arm_validation=predict(preProc_arm,validation[,grep("_arm",colnames(validation))])

##_forearm variables PCA
preProc_forearm=preProcess(train[,grep("_forearm",colnames(train))],method="pca")
pred_forearm=predict(preProc_forearm,train[,grep("_forearm",colnames(train))])
pred_forearm_test=predict(preProc_forearm,test[,grep("_forearm",colnames(test))])
pred_forearm_validation=predict(preProc_forearm,validation[,grep("_forearm",colnames(validation))])

#PCA
preProc=preProcess(train[,grep("_forearm|_arm|_belt|_dumbbell",colnames(train))],method="pca")
pred=predict(preProc,train[,grep("_forearm|_arm|_belt|_dumbbell",colnames(train))])
pred_test=predict(preProc,test[,grep("_forearm|_arm|_belt|_dumbbell",colnames(test))])
pred_validation=predict(preProc,validation[,grep("_forearm|_arm|_belt|_dumbbell",colnames(validation))])



##combine PCAs
trainPCA=data.frame(pred,train$classe)
testPCA=data.frame(pred_test,test$classe)
validationPCA=data.frame(pred_validation,validation$problem_id)
names(testPCA)=names(trainPCA)
names(validationPCA)=names(trainPCA)
```

##Make Prediction Models:
We use rpart classifiction tree, linear discriminant analysis (lda), and naive bayes (nb) tests.  Then we combine all predictors (majority vote).  The most accurate test will be the decider when voting produces no majority. In the end, we see that naive bayes works best with lda in a vey close second.  We set naive bayes as our deciding voter when there is a tie.

```{r, cache=TRUE}
accuracy1=0
accuracy2=0
accuracy3=0


obj1=train(train.classe~.,data=trainsample,method="rpart")
obj2=train(train.classe~.,data=trainsample,method="nb")
obj3=train(train.classe~.,data=trainsample,method="lda")

pred1=predict(obj1,testsample)
pred2=predict(obj2,testsample)
pred3=predict(obj3,testsample)

accuracy1=sum(pred1==testsample$train.classe)/nrow(testsample)
accuracy2=sum(pred2==testsample$train.classe)/nrow(testsample)
accuracy3=sum(pred3==testsample$train.classe)/nrow(testsample)


## We note that test 2 is best, so we will use it in case of no majority for majority vote of data

```

##Predicted Accuracy:

We predict our accuracy of our model by using our test data.  

```{r, cache=TRUE}
##predict on test set
accuracy1=0
accuracy2=0
accuracy3=0
total_accuracy=0
dataset=testPCA
pred1=predict(obj1,dataset)
pred2=predict(obj2,dataset)
pred3=predict(obj3,dataset)
accuracy1=sum(pred1==dataset$train.classe)/nrow(dataset)
accuracy2=sum(pred2==dataset$train.classe)/nrow(dataset)
accuracy3=sum(pred3==dataset$train.classe)/nrow(dataset)
pred13=pred1==pred3
total_pred=ifelse(pred13,as.character(pred1),as.character(pred2))
total_accuracy=total_accuracy+sum(total_pred==as.character(dataset$train.classe))/nrow(dataset)


total_accuracy ##this is predicted accuracy using cross-validation of test set

```

#Run model on test:
I test on our test set.  Our predictions are ouputted below!
```{r}
dataset=testPCA
pred1=predict(obj1,dataset)
pred2=predict(obj2,dataset)
pred3=predict(obj3,dataset)
pred12=pred1==pred2
total_pred=ifelse(pred12,as.character(pred1),as.character(pred3))
total_pred


```
